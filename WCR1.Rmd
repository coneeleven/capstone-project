---
title: "Wine Club Reviews"
author: "Ken Wallace"
date: "May 27, 2016"
output: word_document
---

Need to execute all code snippets in WCR.Rmd before proceeding

# Create a new data frame containing other/additional variables in order to perform linear regression models, trees, and random forests
```{r}
wine.revenue <- wine.sub2 %>% 
  mutate(revenue = sum(total)) %>% 
  group_by(price_per_bottle) %>% 
  select(revenue, total, price, price_per_bottle, bottles, shipprice) #, selcolor, all_around, rating, sale_amt

#wine.revenue$all_around <- as.numeric(wine.revenue$all_around, na.omit(wine.revenue$all_around))
#wine.revenue$rating <- as.double(wine.revenue$rating, na.omit(wine.revenue$rating))
#wine.revenue$sale_amt <- as.double(wine.revenue$sale_amt, na.omit(wine.revenue$sale_amt))

head(wine.revenue, n=20)
```


```{r}
quantile(wine.revenue[,1],probs=c(0.01,0.05,0.5,0.95,0.99),na.rm=T)
```
Error in matrix(unlist(value, recursive = FALSE, use.names = FALSE), nrow = nr,  : 
  length of 'dimnames' [2] not equal to array extent
In addition: There were 50 or more warnings (use warnings() to see the first 50)
 
### don't run this on too many variables (e.g. 6 variables took about 20 minutes) ###
```{r}
library(psych)
pairs.panels(wine.revenue) # The "-1" tells R to read all variables except the first
```



##### The following items are not resolved, and I would like to, but I'm stuck #####

# sales trends over time - i.e. by year, by month/year, and by holiday/non-holiday periods
# I figured out how get it to display year/month, by creating a new column, then arrange by that column (YM). Here is a bar chart, faceted by clubgroup_id. I'm on the right track.

# What I'd like are 2 line charts, where:

# 1. line chart - each clubgroup would be its own line, x would be month/year, and y would be *monthly* revenue, faceted by clubgroup
```{r}

```

# 2. line chart - each clubgroup would be its own line, x would be month/year, and y would be *cumulative* revenue, faceted by clubgroup
```{r}

```

# Then I'd like to figure out the following:
# 1. sales grouped by shipping cost - i.e. 3 buckets (free, under $x, over $x) - need to figure out what x is

# 2. quantity of sales at various price range buckets

# 3. For given session id's, including only ones containing a sale, which other clubs do customers look at before purchasing their chosen club?



######################################################################
### MODELLING STEPS
### It is essential to divide the data set into training and test sets.
### The idea is to develop a model based on the training set,
### then apply this model on the testing set.
### In this case, we can determine both the in-sample and out-of-sample performance metrics

# Split the data into Training and Testing Sets

```{r}
library(randomForest)
library(leaps)
library(tree)
```

# Use this to sample 10% of data
```{r}
set.seed(123)  # The "seed number" is to enable us to do a reproducible research as the
               # random number will be fixed
test <- sample(nrow(wine.sub2),0.3*nrow(wine.sub2))  # Take a random sample of 30% of the data as "test"

data.train <- wine.revenue[-test,]  # All non-test data are classified as part of the training set
data.test <- wine.revenue[test,]    # Testing set

data.train <- sample_frac(data.train, 0.1)
data.test <- sample_frac(data.test, 0.1)

nrow(data.train)
nrow(data.test)
```

# Use this to sample 25% of data
```{r}
set.seed(123)  # The "seed number" is to enable us to do a reproducible research as the
               # random number will be fixed
test <- sample(nrow(wine.sub2),0.3*nrow(wine.sub2))  # Take a random sample of 30% of the data as "test"

data.train <- wine.revenue[-test,]  # All non-test data are classified as part of the training set
data.test <- wine.revenue[test,]    # Testing set

#data.train <- sample_frac(data.train, 0.25)
#data.test <- sample_frac(data.test, 0.25)

nrow(data.train)
nrow(data.test)

```

```{r}
# Make sure we only have the variables we want
# Make sure we only have the variables we want
wine.train.all <- data.train #[,-c(1)]
wine.test.all <- data.test #[,-c(1)]

# This should result in a complete-variable data set
str(wine.train.all)
```

### Linear Models
### (1) Simple Linear Regression (y vs x)
```{r}
lm.1 <- lm(revenue~price+price_per_bottle+bottles+shipprice, data=data.train)
# The lm() function uses the ff arguments: lm(y~x, data = , ...)
# To add more independent variables, use the + sign: y ~ x1 + x2 + ...

# The following provides the results:
lm.1
summary(lm.1)

# Use the model on the training set:
lm1.train <- predict(lm.1,data=data.train)

# Show the square root of the mean squared error (MSE): known actual - predicted
sqrt(mean((lm1.train-data.train$revenue)^2) )

# Similarly, use the model on the testing set and solve for the square root of mean squared error:
lm1.test <- predict(lm.1,data.test)
sqrt(mean((lm1.test-data.test$revenue)^2))
```


### (2) Multiple LR
```{r}
lm.2 <- lm(TARGET_WINS~.-INDEX-wins-TEAM_BATTING_HBP-TEAM_FIELDING_E,data=data.train)
# In the lm() function, use y~. if you want to use all variables;
# Use - to remove independent variables

# In this example, we took out INDEX (not needed), HBP (mostly NAs), TEAM FIELDING E (transformed by log)

lm.2
summary(lm.2)

# Since the result shows NAs and multicollinearity, we further remove the variables causing these:
# For example, wins is not really a predictor but another representation for the dependent
# Also, if we are using TEAM_BASES_EARNED, we have to remove the components that make this up

bb.train <- data.train[,-c(1,3:7,9,10,11,16,18)]
str(bb.train)

lm.2 <- lm(TARGET_WINS~.,data=bb.train)
lm.2
summary(lm.2)

lm2.train <- predict(lm.2,data=data.train)
mean((lm2.train-data.train$TARGET_WINS)^2) # 186.301; 14.68 wins

lm2.test <- predict(lm.2,data.test)
mean((lm2.test-data.test$TARGET_WINS)^2) # 169.0368; 13.89 wins
```


# MACHINE LEARNING REGRESSION ALGORITHMS

# Best Subset Selection

```{r}
library(leaps)
```

# DECISION TREES

```{r}
library(tree)
tree.revenue <- tree(revenue~., data=wine.revenue)
tree.revenue
summary(tree.revenue)
plot(tree.revenue)
text(tree.revenue, pretty=0)
```

```{r}
tree.train <- predict(tree.revenue,data.train)
mean((tree.train-data.train$revenue)^2)  

tree.test <- predict(tree.revenue,data.test)
mean((tree.test-data.test$revenue)^2)   
```

# RANDOM FORESTS

```{r}
rf.1 <- randomForest(revenue~.,data=wine.train.all, importance=TRUE, ntree=500) 
# To specify the number of trees: add the argument "ntree = "  (default = 500)
# To specify the number of variables at each split: "mtry = " (default = m/3 for regression; sqrt(m) for classification)

rf.1   # 
importance(rf.1) # Shows the effect on MSE for each specific variable
varImpPlot(rf.1)

rf.test <- predict(rf.1,wine.test.all)
sqrt(mean((rf.test-data.test$revenue)^2))
```
###############################################################

